{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f7e20f-20e1-4bbd-b6b8-57fd1c6e9e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Reading files from: C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\n",
      "üîç Extracting features from folder...\n",
      "üìÑ Processing: alice_in_wonderland.txt\n",
      "üìÑ Processing: Bumps and His Buddies.txt\n",
      "üìÑ Processing: class.py\n",
      "üìÑ Processing: classtest2.3.py\n",
      "üìÑ Processing: combined_data.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 140\u001b[0m\n\u001b[0;32m    137\u001b[0m output_csv \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmhars\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mNITK Projects\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mIPC\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mExtracted features from files\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfeatures.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÅ Reading files from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_folder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m extract_folder_to_csv(input_folder, output_csv)\n",
      "Cell \u001b[1;32mIn[4], line 113\u001b[0m, in \u001b[0;36mextract_folder_to_csv\u001b[1;34m(folder_path, output_csv)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÑ Processing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m     feats \u001b[38;5;241m=\u001b[39m extract_features(file_path)\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feats:\n\u001b[0;32m    115\u001b[0m         all_features\u001b[38;5;241m.\u001b[39mappend(feats)\n",
      "Cell \u001b[1;32mIn[4], line 88\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     86\u001b[0m feats \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path),\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntropy\u001b[39m\u001b[38;5;124m\"\u001b[39m: calculate_entropy(text),\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUniqueSymbols\u001b[39m\u001b[38;5;124m\"\u001b[39m: unique_symbol_count(text),\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeanRunLength\u001b[39m\u001b[38;5;124m\"\u001b[39m: mean_run_length(text),\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaxRunLength\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_run_length(text),\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbolFreqVariance\u001b[39m\u001b[38;5;124m\"\u001b[39m: symbol_frequency_variance(text),\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBigramRepeatRate\u001b[39m\u001b[38;5;124m\"\u001b[39m: bigram_repeat_rate(text),\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrigramRepeatRate\u001b[39m\u001b[38;5;124m\"\u001b[39m: trigram_repeat_rate(text),\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharacterRepetitionRatio\u001b[39m\u001b[38;5;124m\"\u001b[39m: character_repetition_ratio(text),\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvgWordLength\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_word_length(text),\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhitespaceRatio\u001b[39m\u001b[38;5;124m\"\u001b[39m: whitespace_ratio(text),\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFileSize\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(file_path)\n\u001b[0;32m     99\u001b[0m }\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m feats\n",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m, in \u001b[0;36mcalculate_entropy\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcalculate_entropy\u001b[39m(text):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     freq \u001b[38;5;241m=\u001b[39m Counter(text)\n\u001b[0;32m      9\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n\u001b[0;32m     10\u001b[0m     probs \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;241m/\u001b[39m total \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m freq\u001b[38;5;241m.\u001b[39mvalues()]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\collections\\__init__.py:611\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[1;34m(self, iterable, **kwds)\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m--> 611\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(iterable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## TO EXTRACT FEATURES AND CREATE DATASET FOR THE MODEL\n",
    "import math, os, csv\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- helper feature functions ----------\n",
    "def calculate_entropy(text):\n",
    "    if not text: return 0\n",
    "    freq = Counter(text)\n",
    "    total = len(text)\n",
    "    probs = [f / total for f in freq.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "def unique_symbol_count(text):\n",
    "    return len(set(text)) if text else 0\n",
    "\n",
    "def mean_run_length(text):\n",
    "    if not text: return 0\n",
    "    runs, run = [], 1\n",
    "    for i in range(1, len(text)):\n",
    "        if text[i] == text[i - 1]:\n",
    "            run += 1\n",
    "        else:\n",
    "            runs.append(run)\n",
    "            run = 1\n",
    "    runs.append(run)\n",
    "    return sum(runs) / len(runs)\n",
    "\n",
    "def max_run_length(text):\n",
    "    if not text: return 0\n",
    "    max_run, run = 1, 1\n",
    "    for i in range(1, len(text)):\n",
    "        if text[i] == text[i - 1]:\n",
    "            run += 1\n",
    "            max_run = max(max_run, run)\n",
    "        else:\n",
    "            run = 1\n",
    "    return max_run\n",
    "\n",
    "def symbol_frequency_variance(text):\n",
    "    if not text: return 0\n",
    "    freq = Counter(text)\n",
    "    values = list(freq.values())\n",
    "    mean_val = sum(values) / len(values)\n",
    "    return sum((v - mean_val) ** 2 for v in values) / len(values)\n",
    "\n",
    "def bigram_repeat_rate(text):\n",
    "    if len(text) < 2: return 0\n",
    "    bigrams = [text[i:i+2] for i in range(len(text) - 1)]\n",
    "    freq = Counter(bigrams)\n",
    "    repeats = sum(1 for v in freq.values() if v > 1)\n",
    "    return repeats / len(freq)\n",
    "\n",
    "def trigram_repeat_rate(text):\n",
    "    if len(text) < 3: return 0\n",
    "    trigrams = [text[i:i+3] for i in range(len(text) - 2)]\n",
    "    freq = Counter(trigrams)\n",
    "    repeats = sum(1 for v in freq.values() if v > 1)\n",
    "    return repeats / len(freq)\n",
    "\n",
    "def character_repetition_ratio(text):\n",
    "    if not text: return 0\n",
    "    repeated = sum(1 for i in range(1, len(text)) if text[i] == text[i - 1])\n",
    "    return repeated / len(text)\n",
    "\n",
    "def avg_word_length(text):\n",
    "    words = [w for w in text.split() if w]\n",
    "    if not words: return 0\n",
    "    return sum(len(w) for w in words) / len(words)\n",
    "\n",
    "def whitespace_ratio(text):\n",
    "    if not text: return 0\n",
    "    whites = sum(1 for c in text if c.isspace())\n",
    "    return whites / len(text)\n",
    "\n",
    "# ---------- feature extraction for one file ----------\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    feats = {\n",
    "        \"File\": os.path.basename(file_path),\n",
    "        \"Entropy\": calculate_entropy(text),\n",
    "        \"UniqueSymbols\": unique_symbol_count(text),\n",
    "        \"MeanRunLength\": mean_run_length(text),\n",
    "        \"MaxRunLength\": max_run_length(text),\n",
    "        \"SymbolFreqVariance\": symbol_frequency_variance(text),\n",
    "        \"BigramRepeatRate\": bigram_repeat_rate(text),\n",
    "        \"TrigramRepeatRate\": trigram_repeat_rate(text),\n",
    "        \"CharacterRepetitionRatio\": character_repetition_ratio(text),\n",
    "        \"AvgWordLength\": avg_word_length(text),\n",
    "        \"WhitespaceRatio\": whitespace_ratio(text),\n",
    "        \"FileSize\": os.path.getsize(file_path)\n",
    "    }\n",
    "    return feats\n",
    "\n",
    "# ---------- main folder ‚Üí CSV ----------\n",
    "def extract_folder_to_csv(folder_path, output_csv):\n",
    "    print(\"üîç Extracting features from folder...\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "    all_features = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            print(f\"üìÑ Processing: {filename}\")\n",
    "            feats = extract_features(file_path)\n",
    "            if feats:\n",
    "                all_features.append(feats)\n",
    "\n",
    "    if not all_features:\n",
    "        print(\"‚ö†Ô∏è No valid files found in folder.\")\n",
    "        return\n",
    "\n",
    "    columns = [\n",
    "        \"File\", \"Entropy\", \"UniqueSymbols\", \"MeanRunLength\", \"MaxRunLength\",\n",
    "        \"SymbolFreqVariance\", \"BigramRepeatRate\", \"TrigramRepeatRate\",\n",
    "        \"CharacterRepetitionRatio\", \"AvgWordLength\", \"WhitespaceRatio\", \"FileSize\"\n",
    "    ]\n",
    "\n",
    "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(all_features)\n",
    "\n",
    "    print(f\"\\n‚úÖ Feature extraction complete.\\nSaved to: {output_csv}\\nTotal files processed: {len(all_features)}\")\n",
    "\n",
    "# ---------- run ----------\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\"\n",
    "    output_csv = r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Extracted features from files\\features.csv\"\n",
    "\n",
    "    print(f\"üìÅ Reading files from: {input_folder}\")\n",
    "    extract_folder_to_csv(input_folder, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0e935f5-fa8e-4f38-96d4-c68eb9385cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a96f92b-5865-4652-970c-0c792c4fc880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda5ea4b-0e2c-4cfa-bc7a-08af8a9cbcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to process: ['alice_in_wonderland.txt', 'Bumps and His Buddies.txt', 'Bumps and His Buddies.txt.lzw', 'Bumps and His Buddies_hybrid_compressed.bin', 'class.py', 'classtest2.3.py', 'combined_data.csv', 'c_cpp_properties.json', 'EEG_DATA.csv', 'EEG_DATA.csv.lzw', 'EEG_DATA_hybrid_compressed.bin', 'email_text.txt', 'launch.json', 'Rectangle.py', 'sensor_data.txt', 'sherlock_holmes.txt', 'system_log1.log', 'system_log2.log']\n",
      "[1/18] Processing alice_in_wonderland.txt...\n",
      "[2/18] Processing Bumps and His Buddies.txt...\n",
      "[3/18] Processing Bumps and His Buddies.txt.lzw...\n",
      "[4/18] Processing Bumps and His Buddies_hybrid_compressed.bin...\n",
      "[5/18] Processing class.py...\n",
      "[6/18] Processing classtest2.3.py...\n",
      "[7/18] Processing combined_data.csv...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 184\u001b[0m\n\u001b[0;32m    182\u001b[0m rle_data \u001b[38;5;241m=\u001b[39m rle_compress(data)\n\u001b[0;32m    183\u001b[0m rle_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t\n\u001b[1;32m--> 184\u001b[0m rle_ok \u001b[38;5;241m=\u001b[39m (data \u001b[38;5;241m==\u001b[39m rle_decompress(rle_data))\n\u001b[0;32m    185\u001b[0m rle_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(rle_data)\n\u001b[0;32m    186\u001b[0m rle_ratio \u001b[38;5;241m=\u001b[39m orig_size \u001b[38;5;241m/\u001b[39m rle_size \u001b[38;5;28;01mif\u001b[39;00m rle_size \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m, in \u001b[0;36mrle_decompress\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     23\u001b[0m decompressed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m()\n\u001b[0;32m     24\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m     26\u001b[0m     count \u001b[38;5;241m=\u001b[39m data[i]\n\u001b[0;32m     27\u001b[0m     value \u001b[38;5;241m=\u001b[39m data[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "# ------------------------------\n",
    "# SIMPLE RLE\n",
    "# ------------------------------\n",
    "def rle_compress(data: bytes) -> bytes:\n",
    "    compressed = bytearray()\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        count = 1\n",
    "        while i + 1 < len(data) and data[i] == data[i+1] and count < 255:\n",
    "            i += 1\n",
    "            count += 1\n",
    "        compressed.extend([count, data[i]])\n",
    "        i += 1\n",
    "    return bytes(compressed)\n",
    "\n",
    "def rle_decompress(data: bytes) -> bytes:\n",
    "    decompressed = bytearray()\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        count = data[i]\n",
    "        value = data[i+1]\n",
    "        decompressed.extend([value] * count)\n",
    "        i += 2\n",
    "    return bytes(decompressed)\n",
    "\n",
    "# ------------------------------\n",
    "# FIXED LZW (NO OVERFLOW)\n",
    "# ------------------------------\n",
    "def lzw_compress(data: bytes) -> bytes:\n",
    "    dictionary = {bytes([i]): i for i in range(256)}\n",
    "    w = b\"\"\n",
    "    result = []\n",
    "    code = 256\n",
    "\n",
    "    for c in data:\n",
    "        wc = w + bytes([c])\n",
    "        if wc in dictionary:\n",
    "            w = wc\n",
    "        else:\n",
    "            result.append(dictionary[w])\n",
    "            dictionary[wc] = code\n",
    "            code += 1\n",
    "            w = bytes([c])\n",
    "\n",
    "    if w:\n",
    "        result.append(dictionary[w])\n",
    "\n",
    "    # Determine how many bytes needed\n",
    "    max_code = max(result)\n",
    "    if max_code <= 0xFFFF:\n",
    "        width = 2\n",
    "    elif max_code <= 0xFFFFFF:\n",
    "        width = 3\n",
    "    else:\n",
    "        width = 4\n",
    "\n",
    "    compressed = bytearray([width])  # store width\n",
    "\n",
    "    for num in result:\n",
    "        compressed.extend(num.to_bytes(width, \"big\"))\n",
    "\n",
    "    return bytes(compressed)\n",
    "\n",
    "def lzw_decompress(data: bytes) -> bytes:\n",
    "    width = data[0]      # read width\n",
    "    data = data[1:]\n",
    "\n",
    "    codes = [\n",
    "        int.from_bytes(data[i:i+width], \"big\")\n",
    "        for i in range(0, len(data), width)\n",
    "    ]\n",
    "\n",
    "    dictionary = {i: bytes([i]) for i in range(256)}\n",
    "    code = 256\n",
    "\n",
    "    w = bytes([codes[0]])\n",
    "    result = bytearray(w)\n",
    "\n",
    "    for k in codes[1:]:\n",
    "        if k in dictionary:\n",
    "            entry = dictionary[k]\n",
    "        elif k == code:\n",
    "            entry = w + w[:1]\n",
    "        else:\n",
    "            raise ValueError(\"Bad LZW code\")\n",
    "\n",
    "        result.extend(entry)\n",
    "        dictionary[code] = w + entry[:1]\n",
    "        code += 1\n",
    "        w = entry\n",
    "\n",
    "    return bytes(result)\n",
    "\n",
    "# ------------------------------\n",
    "# SIMPLE HUFFMAN\n",
    "# ------------------------------\n",
    "class HuffmanNode:\n",
    "    def __init__(self, freq, byte=None, left=None, right=None):\n",
    "        self.freq = freq\n",
    "        self.byte = byte\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "\n",
    "def build_huffman_tree(data: bytes):\n",
    "    counter = Counter(data)\n",
    "    heap = [HuffmanNode(freq, b) for b, freq in counter.items()]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap) > 1:\n",
    "        n1 = heapq.heappop(heap)\n",
    "        n2 = heapq.heappop(heap)\n",
    "        heapq.heappush(heap, HuffmanNode(n1.freq + n2.freq, None, n1, n2))\n",
    "\n",
    "    return heap[0]\n",
    "\n",
    "def build_huffman_codes(node, prefix=\"\", codebook=None):\n",
    "    if codebook is None:\n",
    "        codebook = {}\n",
    "    if node.byte is not None:\n",
    "        codebook[node.byte] = prefix\n",
    "    else:\n",
    "        build_huffman_codes(node.left, prefix + \"0\", codebook)\n",
    "        build_huffman_codes(node.right, prefix + \"1\", codebook)\n",
    "    return codebook\n",
    "\n",
    "def huffman_compress(data: bytes):\n",
    "    tree = build_huffman_tree(data)\n",
    "    codebook = build_huffman_codes(tree)\n",
    "    bitstring = \"\".join(codebook[b] for b in data)\n",
    "    extra = (8 - len(bitstring) % 8) % 8\n",
    "    bitstring += \"0\" * extra\n",
    "    compressed = bytearray(int(bitstring[i:i+8], 2) for i in range(0, len(bitstring), 8))\n",
    "    return bytes(compressed), codebook\n",
    "\n",
    "def huffman_decompress(compressed: bytes, codebook: dict) -> bytes:\n",
    "    rev = {v: bytes([k]) for k, v in codebook.items()}\n",
    "    bitstring = \"\".join(f\"{b:08b}\" for b in compressed)\n",
    "    result = bytearray()\n",
    "\n",
    "    code = \"\"\n",
    "    for bit in bitstring:\n",
    "        code += bit\n",
    "        if code in rev:\n",
    "            result.extend(rev[code])\n",
    "            code = \"\"\n",
    "    return bytes(result)\n",
    "\n",
    "# ------------------------------\n",
    "# DIRECTORIES\n",
    "# ------------------------------\n",
    "INPUT_FOLDER = r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\"\n",
    "OUTPUT_CSV = r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Compressed info of different files\\compression_results.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_CSV), exist_ok=True)\n",
    "\n",
    "files = [f for f in os.listdir(INPUT_FOLDER) if os.path.isfile(os.path.join(INPUT_FOLDER,f))]\n",
    "print(\"Files to process:\", files)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# ------------------------------\n",
    "# PROCESS ALL FILES\n",
    "# ------------------------------\n",
    "for i, fname in enumerate(files, 1):\n",
    "    print(f\"[{i}/{len(files)}] Processing {fname}...\")\n",
    "    fpath = os.path.join(INPUT_FOLDER, fname)\n",
    "\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    orig_size = len(data)\n",
    "\n",
    "    # -------- RLE --------\n",
    "    t = time.time()\n",
    "    rle_data = rle_compress(data)\n",
    "    rle_time = time.time() - t\n",
    "    rle_ok = (data == rle_decompress(rle_data))\n",
    "    rle_size = len(rle_data)\n",
    "    rle_ratio = orig_size / rle_size if rle_size else 0\n",
    "\n",
    "    # -------- LZW --------\n",
    "    t = time.time()\n",
    "    lzw_data = lzw_compress(data)\n",
    "    lzw_time = time.time() - t\n",
    "    lzw_ok = (data == lzw_decompress(lzw_data))\n",
    "    lzw_size = len(lzw_data)\n",
    "    lzw_ratio = orig_size / lzw_size if lzw_size else 0\n",
    "\n",
    "    # -------- Huffman --------\n",
    "    t = time.time()\n",
    "    huff_data, codebook = huffman_compress(data)\n",
    "    huff_time = time.time() - t\n",
    "    huff_ok = (data == huffman_decompress(huff_data, codebook))\n",
    "    huff_size = len(huff_data)\n",
    "    huff_ratio = orig_size / huff_size if huff_size else 0\n",
    "\n",
    "    # -------- BEST --------\n",
    "    size_dict = {\n",
    "        \"RLE\": rle_size if rle_ok else float('inf'),\n",
    "        \"LZW\": lzw_size if lzw_ok else float('inf'),\n",
    "        \"Huffman\": huff_size if huff_ok else float('inf')\n",
    "    }\n",
    "    ratio_dict = {\n",
    "        \"RLE\": rle_ratio if rle_ok else 0,\n",
    "        \"LZW\": lzw_ratio if lzw_ok else 0,\n",
    "        \"Huffman\": huff_ratio if huff_ok else 0\n",
    "    }\n",
    "\n",
    "    best_size = min(size_dict, key=size_dict.get)\n",
    "    best_ratio = max(ratio_dict, key=ratio_dict.get)\n",
    "\n",
    "    # -------- STORE RESULT --------\n",
    "    rows.append({\n",
    "        \"file\": fname,\n",
    "        \"original_size\": orig_size,\n",
    "        \"RLE_size\": rle_size,\n",
    "        \"RLE_ratio\": rle_ratio,\n",
    "        \"RLE_time\": rle_time,\n",
    "        \"RLE_ok\": rle_ok,\n",
    "        \"LZW_size\": lzw_size,\n",
    "        \"LZW_ratio\": lzw_ratio,\n",
    "        \"LZW_time\": lzw_time,\n",
    "        \"LZW_ok\": lzw_ok,\n",
    "        \"Huffman_size\": huff_size,\n",
    "        \"Huffman_ratio\": huff_ratio,\n",
    "        \"Huffman_time\": huff_time,\n",
    "        \"Huffman_ok\": huff_ok,\n",
    "        \"best_by_size\": best_size,\n",
    "        \"best_by_ratio\": best_ratio\n",
    "    })\n",
    "\n",
    "# ------------------------------\n",
    "# SAVE CSV\n",
    "# ------------------------------\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"Compression results saved to {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea1c410-96bc-4238-b4ee-9d5ec59f8d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV saved to: C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\final_merged_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load both datasets\n",
    "features_df = pd.read_csv(r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Extracted features from files\\features.csv\")\n",
    "compression_df = pd.read_csv(r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Compressed info of different files\\compression_results.csv\")\n",
    "\n",
    "# Merge on filename\n",
    "final_df = pd.merge(features_df, compression_df, on=\"file\", how=\"inner\")\n",
    "\n",
    "# Save final merged file\n",
    "output_path = r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\final_merged_results.csv\"\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Merged CSV saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e070f-391f-4f37-b2b7-692988349879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# ================= LOAD DATA =================\n",
    "df = pd.read_csv(r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\final_merged_results.csv\")\n",
    "\n",
    "# strip column names (just in case)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# ========= SELECT TARGET LABEL HERE =========\n",
    "target = \"best_by_ratio\"     # or \"best_by_size\"\n",
    "\n",
    "# ========= EXTRACT TARGET BEFORE DROPPING =========\n",
    "y = df[target]\n",
    "df = df.drop(columns=[target])\n",
    "\n",
    "# ========= DROP NON-NUMERIC METADATA =========\n",
    "df = df.drop(columns=[\"file\"], errors=\"ignore\")\n",
    "\n",
    "# ========= DROP COMPRESSION RESULT COLUMNS =========\n",
    "bad_cols = [c for c in df.columns if \n",
    "            \"size\" in c.lower() or\n",
    "            \"ratio\" in c.lower() or\n",
    "            \"time\" in c.lower() or\n",
    "            \"_ok\" in c.lower()\n",
    "           ]\n",
    "\n",
    "df = df.drop(columns=bad_cols, errors=\"ignore\")\n",
    "\n",
    "# ========= ENSURE ALL FEATURES ARE NUMERIC =========\n",
    "X = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "\n",
    "# truncate y to match X length\n",
    "y = y.iloc[X.index]\n",
    "\n",
    "# ========= ENCODE TARGET =========\n",
    "label_enc = LabelEncoder()\n",
    "y = label_enc.fit_transform(y)\n",
    "\n",
    "# ========= TRAIN TEST SPLIT =========\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ========= TRAIN RANDOM FOREST =========\n",
    "model = RandomForestClassifier(n_estimators=300)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ========= EVALUATE =========\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nReport:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ========= LABEL MAPPING =========\n",
    "print(\"\\nLabel mapping:\")\n",
    "print(dict(zip(label_enc.classes_, label_enc.transform(label_enc.classes_))))\n",
    "\n",
    "# ========= SAVE MODEL =========\n",
    "joblib.dump(model, \"best_algo_model.pkl\")\n",
    "joblib.dump(label_enc, \"label_encoder.pkl\")\n",
    "joblib.dump(X.columns.tolist(), \"feature_list.pkl\")\n",
    "\n",
    "print(\"\\nModel saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a181632-cd20-4dfe-9b63-dea4df436218",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO EXTRACT FEATURES FROM OUR TEST FILE\n",
    "\n",
    "## TO EXTRACT FEATURES AND CREATE DATASET FOR THE MODEL\n",
    "import math, os, csv\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- helper feature functions ----------\n",
    "def calculate_entropy(text):\n",
    "    if not text: return 0\n",
    "    freq = Counter(text)\n",
    "    total = len(text)\n",
    "    probs = [f / total for f in freq.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "def unique_symbol_count(text):\n",
    "    return len(set(text)) if text else 0\n",
    "\n",
    "def mean_run_length(text):\n",
    "    if not text: return 0\n",
    "    runs, run = [], 1\n",
    "    for i in range(1, len(text)):\n",
    "        if text[i] == text[i - 1]:\n",
    "            run += 1\n",
    "        else:\n",
    "            runs.append(run)\n",
    "            run = 1\n",
    "    runs.append(run)\n",
    "    return sum(runs) / len(runs)\n",
    "\n",
    "def max_run_length(text):\n",
    "    if not text: return 0\n",
    "    max_run, run = 1, 1\n",
    "    for i in range(1, len(text)):\n",
    "        if text[i] == text[i - 1]:\n",
    "            run += 1\n",
    "            max_run = max(max_run, run)\n",
    "        else:\n",
    "            run = 1\n",
    "    return max_run\n",
    "\n",
    "def symbol_frequency_variance(text):\n",
    "    if not text: return 0\n",
    "    freq = Counter(text)\n",
    "    values = list(freq.values())\n",
    "    mean_val = sum(values) / len(values)\n",
    "    return sum((v - mean_val) ** 2 for v in values) / len(values)\n",
    "\n",
    "def bigram_repeat_rate(text):\n",
    "    if len(text) < 2: return 0\n",
    "    bigrams = [text[i:i+2] for i in range(len(text) - 1)]\n",
    "    freq = Counter(bigrams)\n",
    "    repeats = sum(1 for v in freq.values() if v > 1)\n",
    "    return repeats / len(freq)\n",
    "\n",
    "def trigram_repeat_rate(text):\n",
    "    if len(text) < 3: return 0\n",
    "    trigrams = [text[i:i+3] for i in range(len(text) - 2)]\n",
    "    freq = Counter(trigrams)\n",
    "    repeats = sum(1 for v in freq.values() if v > 1)\n",
    "    return repeats / len(freq)\n",
    "\n",
    "def character_repetition_ratio(text):\n",
    "    if not text: return 0\n",
    "    repeated = sum(1 for i in range(1, len(text)) if text[i] == text[i - 1])\n",
    "    return repeated / len(text)\n",
    "\n",
    "def avg_word_length(text):\n",
    "    words = [w for w in text.split() if w]\n",
    "    if not words: return 0\n",
    "    return sum(len(w) for w in words) / len(words)\n",
    "\n",
    "def whitespace_ratio(text):\n",
    "    if not text: return 0\n",
    "    whites = sum(1 for c in text if c.isspace())\n",
    "    return whites / len(text)\n",
    "\n",
    "\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "    if not text:\n",
    "        return None\n",
    "\n",
    "    feats = {\n",
    "        \"File\": os.path.basename(file_path),\n",
    "        \"Entropy\": calculate_entropy(text),\n",
    "        \"UniqueSymbols\": unique_symbol_count(text),\n",
    "        \"MeanRunLength\": mean_run_length(text),\n",
    "        \"MaxRunLength\": max_run_length(text),\n",
    "        \"SymbolFreqVariance\": symbol_frequency_variance(text),\n",
    "        \"BigramRepeatRate\": bigram_repeat_rate(text),\n",
    "        \"TrigramRepeatRate\": trigram_repeat_rate(text),\n",
    "        \"CharacterRepetitionRatio\": character_repetition_ratio(text),\n",
    "        \"AvgWordLength\": avg_word_length(text),\n",
    "        \"WhitespaceRatio\": whitespace_ratio(text),\n",
    "        \"FileSize\": os.path.getsize(file_path)\n",
    "    }\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23aaaed2-eaa8-42fd-8adc-c85250deb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TO COMPRESS THE DATA ALL THREE COMPRESSORS\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "# ------------------------------\n",
    "# SIMPLE RLE\n",
    "# ------------------------------\n",
    "def rle_compress(data: bytes) -> bytes:\n",
    "    compressed = bytearray()\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        count = 1\n",
    "        while i + 1 < len(data) and data[i] == data[i+1] and count < 255:\n",
    "            i += 1\n",
    "            count += 1\n",
    "        compressed.extend([count, data[i]])\n",
    "        i += 1\n",
    "    return bytes(compressed)\n",
    "\n",
    "def rle_decompress(data: bytes) -> bytes:\n",
    "    decompressed = bytearray()\n",
    "    i = 0\n",
    "    while i < len(data):\n",
    "        count = data[i]\n",
    "        value = data[i+1]\n",
    "        decompressed.extend([value] * count)\n",
    "        i += 2\n",
    "    return bytes(decompressed)\n",
    "\n",
    "# ------------------------------\n",
    "# FIXED LZW (NO OVERFLOW)\n",
    "# ------------------------------\n",
    "def lzw_compress(data: bytes) -> bytes:\n",
    "    dictionary = {bytes([i]): i for i in range(256)}\n",
    "    w = b\"\"\n",
    "    result = []\n",
    "    code = 256\n",
    "\n",
    "    for c in data:\n",
    "        wc = w + bytes([c])\n",
    "        if wc in dictionary:\n",
    "            w = wc\n",
    "        else:\n",
    "            result.append(dictionary[w])\n",
    "            dictionary[wc] = code\n",
    "            code += 1\n",
    "            w = bytes([c])\n",
    "\n",
    "    if w:\n",
    "        result.append(dictionary[w])\n",
    "\n",
    "    # Determine how many bytes needed\n",
    "    max_code = max(result)\n",
    "    if max_code <= 0xFFFF:\n",
    "        width = 2\n",
    "    elif max_code <= 0xFFFFFF:\n",
    "        width = 3\n",
    "    else:\n",
    "        width = 4\n",
    "\n",
    "    compressed = bytearray([width])  # store width\n",
    "\n",
    "    for num in result:\n",
    "        compressed.extend(num.to_bytes(width, \"big\"))\n",
    "\n",
    "    return bytes(compressed)\n",
    "\n",
    "\n",
    "def lzw_decompress(data: bytes) -> bytes:\n",
    "    width = data[0]      # read width\n",
    "    data = data[1:]\n",
    "\n",
    "    codes = [\n",
    "        int.from_bytes(data[i:i+width], \"big\")\n",
    "        for i in range(0, len(data), width)\n",
    "    ]\n",
    "\n",
    "    dictionary = {i: bytes([i]) for i in range(256)}\n",
    "    code = 256\n",
    "\n",
    "    w = bytes([codes[0]])\n",
    "    result = bytearray(w)\n",
    "\n",
    "    for k in codes[1:]:\n",
    "        if k in dictionary:\n",
    "            entry = dictionary[k]\n",
    "        elif k == code:\n",
    "            entry = w + w[:1]\n",
    "        else:\n",
    "            raise ValueError(\"Bad LZW code\")\n",
    "\n",
    "        result.extend(entry)\n",
    "        dictionary[code] = w + entry[:1]\n",
    "        code += 1\n",
    "        w = entry\n",
    "\n",
    "    return bytes(result)\n",
    "\n",
    "# ------------------------------\n",
    "# SIMPLE HUFFMAN\n",
    "# ------------------------------\n",
    "class HuffmanNode:\n",
    "    def __init__(self, freq, byte=None, left=None, right=None):\n",
    "        self.freq = freq\n",
    "        self.byte = byte\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "    def __lt__(self, other):\n",
    "        return self.freq < other.freq\n",
    "\n",
    "def build_huffman_tree(data: bytes):\n",
    "    counter = Counter(data)\n",
    "    heap = [HuffmanNode(freq, b) for b, freq in counter.items()]\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap) > 1:\n",
    "        n1 = heapq.heappop(heap)\n",
    "        n2 = heapq.heappop(heap)\n",
    "        heapq.heappush(heap, HuffmanNode(n1.freq + n2.freq, None, n1, n2))\n",
    "\n",
    "    return heap[0]\n",
    "\n",
    "def build_huffman_codes(node, prefix=\"\", codebook=None):\n",
    "    if codebook is None:\n",
    "        codebook = {}\n",
    "    if node.byte is not None:\n",
    "        codebook[node.byte] = prefix\n",
    "    else:\n",
    "        build_huffman_codes(node.left, prefix + \"0\", codebook)\n",
    "        build_huffman_codes(node.right, prefix + \"1\", codebook)\n",
    "    return codebook\n",
    "\n",
    "def huffman_compress(data: bytes):\n",
    "    tree = build_huffman_tree(data)\n",
    "    codebook = build_huffman_codes(tree)\n",
    "    bitstring = \"\".join(codebook[b] for b in data)\n",
    "    extra = (8 - len(bitstring) % 8) % 8\n",
    "    bitstring += \"0\" * extra\n",
    "    compressed = bytearray(int(bitstring[i:i+8], 2) for i in range(0, len(bitstring), 8))\n",
    "    return bytes(compressed), codebook\n",
    "\n",
    "def huffman_decompress(compressed: bytes, codebook: dict) -> bytes:\n",
    "    rev = {v: bytes([k]) for k, v in codebook.items()}\n",
    "    bitstring = \"\".join(f\"{b:08b}\" for b in compressed)\n",
    "    result = bytearray()\n",
    "\n",
    "    code = \"\"\n",
    "    for bit in bitstring:\n",
    "        code += bit\n",
    "        if code in rev:\n",
    "            result.extend(rev[code])\n",
    "            code = \"\"\n",
    "    return bytes(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4e87ab9-59a5-4cbc-b132-2ad2b4dd2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTING USING OUR FILE\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# ========= LOAD TRAINED MODEL + ENCODER =========\n",
    "model = joblib.load(\"best_algo_model.pkl\")\n",
    "label_enc = joblib.load(\"label_encoder.pkl\")\n",
    "feature_list = joblib.load(\"feature_list.pkl\")\n",
    "\n",
    "\n",
    "# ========= FINAL PREDICT + COMPRESS FUNCTION =========\n",
    "\n",
    "def predict_and_compress(file_path):\n",
    "\n",
    "    # ---- READ THE FILE ----\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "\n",
    "    # ---- Extract features using YOUR function ----\n",
    "    feats = extract_features(file_path)\n",
    "    if feats is None:\n",
    "        print(\"‚ùå Could not extract features.\")\n",
    "        return\n",
    "\n",
    "    # ---- Convert to DataFrame with correct column order ----\n",
    "    X_new = pd.DataFrame([feats])[feature_list]\n",
    "\n",
    "    # ---- Predict best algorithm ----\n",
    "    pred = model.predict(X_new)[0]\n",
    "    algo = label_enc.inverse_transform([pred])[0]\n",
    "\n",
    "    print(\"Best algorithm predicted:\", algo)\n",
    "\n",
    "    # ---- Run correct compression ----\n",
    "    if algo == \"RLE\":\n",
    "        compressed = rle_compress(raw_data)\n",
    "        out_file = file_path + \".rle\"\n",
    "\n",
    "    elif algo == \"LZW\":\n",
    "        compressed = lzw_compress(raw_data)\n",
    "        out_file = file_path + \".lzw\"\n",
    "\n",
    "    elif algo == \"Huffman\":\n",
    "        compressed, _ = huffman_compress(raw_data)\n",
    "        out_file = file_path + \".huff\"\n",
    "\n",
    "    # ---- Save compressed output ----\n",
    "    with open(out_file, \"wb\") as f:\n",
    "        f.write(compressed)\n",
    "\n",
    "    print(\"Compressed file saved as:\", out_file)\n",
    "    print(\"Original size:\", len(raw_data), \"bytes\")\n",
    "    print(\"Compressed size:\", len(compressed), \"bytes\")\n",
    "    print(\"Compression ratio:\", len(raw_data) / len(compressed))\n",
    "\n",
    "    return algo, out_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8410bf06-1ae9-4d73-8da3-e12a94546202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d33a631-9588-4014-ab00-2a4a2e50957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def hybrid_rle_lzw_huffman(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    original_size = len(data)\n",
    "\n",
    "    print(\"STEP 1: RLE Compression...\")\n",
    "    rle_data = rle_compress(data)\n",
    "    rle_size = len(rle_data)\n",
    "    rle_ratio = original_size / rle_size if rle_size else 0\n",
    "\n",
    "    print(f\" - RLE compressed size: {rle_size} bytes\")\n",
    "    print(f\" - RLE compression ratio: {rle_ratio}\")\n",
    "\n",
    "    print(\"\\nSTEP 2: LZW Compression on RLE output...\")\n",
    "    lzw_data = lzw_compress(rle_data)\n",
    "    lzw_size = len(lzw_data)\n",
    "    lzw_ratio = original_size / lzw_size if lzw_size else 0\n",
    "\n",
    "    print(f\" - LZW compressed size: {lzw_size} bytes\")\n",
    "    print(f\" - LZW compression ratio: {lzw_ratio}\")\n",
    "\n",
    "    print(\"\\nSTEP 3: Huffman Compression on LZW output...\")\n",
    "    huff_data, codebook = huffman_compress(lzw_data)\n",
    "    huff_size = len(huff_data)\n",
    "    huff_ratio = original_size / huff_size if huff_size else 0\n",
    "\n",
    "    print(f\" - Huffman compressed size: {huff_size} bytes\")\n",
    "    print(f\" - Huffman compression ratio: {huff_ratio}\")\n",
    "\n",
    "    # ==== SAVE ONLY FINAL COMPRESSED FILE ====\n",
    "    save_path = os.path.splitext(file_path)[0] + \"_hybrid_compressed.bin\"\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(huff_data)\n",
    "\n",
    "    print(f\"\\nFinal compressed file saved as:\\n {save_path}\")\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\" FINAL HYBRID COMPRESSION RESULT \")\n",
    "    print(\"==============================\")\n",
    "    print(f\"Original size: {original_size} bytes\")\n",
    "    print(f\"Final compressed size: {huff_size} bytes\")\n",
    "    print(f\"Final compression ratio: {huff_ratio}\")\n",
    "\n",
    "    return {\n",
    "        \"original_size\": original_size,\n",
    "        \"after_rle_size\": rle_size,\n",
    "        \"after_lzw_size\": lzw_size,\n",
    "        \"final_huffman_size\": huff_size,\n",
    "        \"final_ratio\": huff_ratio,\n",
    "        \"compressed_file\": save_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dea463ed-71d9-455b-a3cb-89849f2e39b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best algorithm predicted: LZW\n",
      "Compressed file saved as: C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\Bumps and His Buddies.txt.lzw\n",
      "Original size: 76295 bytes\n",
      "Compressed size: 41197 bytes\n",
      "Compression ratio: 1.8519552394591838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('LZW',\n",
       " 'C:\\\\Users\\\\mhars\\\\Desktop\\\\NITK Projects\\\\IPC\\\\Files\\\\Bumps and His Buddies.txt.lzw')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST1 ON .TXT FILE USING PROPOSED METHOD\n",
    "\n",
    "predict_and_compress(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\Bumps and His Buddies.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a247e6b6-fbd8-4588-95c3-0b938923c7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: RLE Compression...\n",
      " - RLE compressed size: 148370 bytes\n",
      " - RLE compression ratio: 0.5142212037473883\n",
      "\n",
      "STEP 2: LZW Compression on RLE output...\n",
      " - LZW compressed size: 48687 bytes\n",
      " - LZW compression ratio: 1.5670507527676794\n",
      "\n",
      "STEP 3: Huffman Compression on LZW output...\n",
      " - Huffman compressed size: 45853 bytes\n",
      " - Huffman compression ratio: 1.6639042156456503\n",
      "\n",
      "Final compressed file saved as:\n",
      " C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\Bumps and His Buddies_hybrid_compressed.bin\n",
      "\n",
      "==============================\n",
      " FINAL HYBRID COMPRESSION RESULT \n",
      "==============================\n",
      "Original size: 76295 bytes\n",
      "Final compressed size: 45853 bytes\n",
      "Final compression ratio: 1.6639042156456503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_size': 76295,\n",
       " 'after_rle_size': 148370,\n",
       " 'after_lzw_size': 48687,\n",
       " 'final_huffman_size': 45853,\n",
       " 'final_ratio': 1.6639042156456503,\n",
       " 'compressed_file': 'C:\\\\Users\\\\mhars\\\\Desktop\\\\NITK Projects\\\\IPC\\\\Files\\\\Bumps and His Buddies_hybrid_compressed.bin'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST1 ON .TXT FILE USING HYBRID METHOD\n",
    "\n",
    "hybrid_rle_lzw_huffman(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\Bumps and His Buddies.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6491cf50-afc8-441a-997b-0f0f71cc7b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best algorithm predicted: LZW\n",
      "Compressed file saved as: C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\EEG_DATA.csv.lzw\n",
      "Original size: 151743556 bytes\n",
      "Compressed size: 78020477 bytes\n",
      "Compression ratio: 1.9449196138598333\n",
      "STEP 1: RLE Compression...\n",
      " - RLE compressed size: 291224492 bytes\n",
      " - RLE compression ratio: 0.521053552048088\n",
      "\n",
      "STEP 2: LZW Compression on RLE output...\n",
      " - LZW compressed size: 83559345 bytes\n",
      " - LZW compression ratio: 1.8159974327228152\n",
      "\n",
      "STEP 3: Huffman Compression on LZW output...\n",
      " - Huffman compressed size: 71113585 bytes\n",
      " - Huffman compression ratio: 2.133819522669262\n",
      "\n",
      "Final compressed file saved as:\n",
      " C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\EEG_DATA_hybrid_compressed.bin\n",
      "\n",
      "==============================\n",
      " FINAL HYBRID COMPRESSION RESULT \n",
      "==============================\n",
      "Original size: 151743556 bytes\n",
      "Final compressed size: 71113585 bytes\n",
      "Final compression ratio: 2.133819522669262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_size': 151743556,\n",
       " 'after_rle_size': 291224492,\n",
       " 'after_lzw_size': 83559345,\n",
       " 'final_huffman_size': 71113585,\n",
       " 'final_ratio': 2.133819522669262,\n",
       " 'compressed_file': 'C:\\\\Users\\\\mhars\\\\Desktop\\\\NITK Projects\\\\IPC\\\\Files\\\\EEG_DATA_hybrid_compressed.bin'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TEST2 ON CSV FILE\n",
    "\n",
    "predict_and_compress(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\EEG_DATA.csv\"\n",
    ")\n",
    "hybrid_rle_lzw_huffman(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\EEG_DATA.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1022da7-f70c-4498-9eeb-c35d67d7bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best algorithm predicted: Huffman\n",
      "Compressed file saved as: C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\class.py.huff\n",
      "Original size: 552 bytes\n",
      "Compressed size: 325 bytes\n",
      "Compression ratio: 1.6984615384615385\n",
      "STEP 1: RLE Compression...\n",
      " - RLE compressed size: 956 bytes\n",
      " - RLE compression ratio: 0.5774058577405857\n",
      "\n",
      "STEP 2: LZW Compression on RLE output...\n",
      " - LZW compressed size: 807 bytes\n",
      " - LZW compression ratio: 0.6840148698884758\n",
      "\n",
      "STEP 3: Huffman Compression on LZW output...\n",
      " - Huffman compressed size: 491 bytes\n",
      " - Huffman compression ratio: 1.124236252545825\n",
      "\n",
      "Final compressed file saved as:\n",
      " C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\class_hybrid_compressed.bin\n",
      "\n",
      "==============================\n",
      " FINAL HYBRID COMPRESSION RESULT \n",
      "==============================\n",
      "Original size: 552 bytes\n",
      "Final compressed size: 491 bytes\n",
      "Final compression ratio: 1.124236252545825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_size': 552,\n",
       " 'after_rle_size': 956,\n",
       " 'after_lzw_size': 807,\n",
       " 'final_huffman_size': 491,\n",
       " 'final_ratio': 1.124236252545825,\n",
       " 'compressed_file': 'C:\\\\Users\\\\mhars\\\\Desktop\\\\NITK Projects\\\\IPC\\\\Files\\\\class_hybrid_compressed.bin'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TEST3 ON .py FILE\n",
    "\n",
    "predict_and_compress(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\class.py\"\n",
    ")\n",
    "hybrid_rle_lzw_huffman(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\class.py\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fd40cb7-bfee-49eb-a0de-a01e3f4c224a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best algorithm predicted: Huffman\n",
      "Compressed file saved as: C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\system_log1.log.huff\n",
      "Original size: 996 bytes\n",
      "Compressed size: 631 bytes\n",
      "Compression ratio: 1.578446909667195\n",
      "STEP 1: RLE Compression...\n",
      " - RLE compressed size: 1862 bytes\n",
      " - RLE compression ratio: 0.5349087003222341\n",
      "\n",
      "STEP 2: LZW Compression on RLE output...\n",
      " - LZW compressed size: 1293 bytes\n",
      " - LZW compression ratio: 0.7703016241299304\n",
      "\n",
      "STEP 3: Huffman Compression on LZW output...\n",
      " - Huffman compressed size: 856 bytes\n",
      " - Huffman compression ratio: 1.1635514018691588\n",
      "\n",
      "Final compressed file saved as:\n",
      " C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\system_log1_hybrid_compressed.bin\n",
      "\n",
      "==============================\n",
      " FINAL HYBRID COMPRESSION RESULT \n",
      "==============================\n",
      "Original size: 996 bytes\n",
      "Final compressed size: 856 bytes\n",
      "Final compression ratio: 1.1635514018691588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_size': 996,\n",
       " 'after_rle_size': 1862,\n",
       " 'after_lzw_size': 1293,\n",
       " 'final_huffman_size': 856,\n",
       " 'final_ratio': 1.1635514018691588,\n",
       " 'compressed_file': 'C:\\\\Users\\\\mhars\\\\Desktop\\\\NITK Projects\\\\IPC\\\\Files\\\\system_log1_hybrid_compressed.bin'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TEST4 ON .log FILE\n",
    "\n",
    "predict_and_compress(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\system_log1.log\"\n",
    ")\n",
    "hybrid_rle_lzw_huffman(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\system_log1.log\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe0db5d-c239-4bdd-b7c7-d8cad9b5030b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best algorithm predicted: Huffman\n",
      "Compressed file saved as: C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\launch.json.huff\n",
      "Original size: 609 bytes\n",
      "Compressed size: 342 bytes\n",
      "Compression ratio: 1.780701754385965\n",
      "STEP 1: RLE Compression...\n",
      " - RLE compressed size: 978 bytes\n",
      " - RLE compression ratio: 0.6226993865030674\n",
      "\n",
      "STEP 2: LZW Compression on RLE output...\n",
      " - LZW compressed size: 853 bytes\n",
      " - LZW compression ratio: 0.7139507620164126\n",
      "\n",
      "STEP 3: Huffman Compression on LZW output...\n",
      " - Huffman compressed size: 509 bytes\n",
      " - Huffman compression ratio: 1.1964636542239686\n",
      "\n",
      "Final compressed file saved as:\n",
      " C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\launch_hybrid_compressed.bin\n",
      "\n",
      "==============================\n",
      " FINAL HYBRID COMPRESSION RESULT \n",
      "==============================\n",
      "Original size: 609 bytes\n",
      "Final compressed size: 509 bytes\n",
      "Final compression ratio: 1.1964636542239686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'original_size': 609,\n",
       " 'after_rle_size': 978,\n",
       " 'after_lzw_size': 853,\n",
       " 'final_huffman_size': 509,\n",
       " 'final_ratio': 1.1964636542239686,\n",
       " 'compressed_file': 'C:\\\\Users\\\\mhars\\\\Desktop\\\\NITK Projects\\\\IPC\\\\Files\\\\launch_hybrid_compressed.bin'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##TEST4 ON .log FILE\n",
    "\n",
    "predict_and_compress(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\launch.json\"\n",
    ")\n",
    "hybrid_rle_lzw_huffman(\n",
    "    r\"C:\\Users\\mhars\\Desktop\\NITK Projects\\IPC\\Files\\launch.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13132de3-bfbf-4687-be5e-4c05e46633af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
